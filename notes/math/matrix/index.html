<!DOCTYPE html>
<html lang="en-gb">

<head>
  <meta name="generator" content="Hugo 0.80.0" />
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Matrizen | Escape Velocity</title>

  
  
  
  
  
  

  

  <meta name="author" content="Christina Unger">


  <meta property="og:title" content="Matrizen" />
<meta property="og:description" content="Matrizen, deren Eigenschaften und Interpretationen" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://cunger.github.io/notes/math/matrix/" />
<meta property="article:published_time" content="2020-08-26T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-08-26T00:00:00+00:00" />

  




  
  
  
  
  

  <link rel="canonical" href="http://cunger.github.io/notes/math/matrix/">  

  <link href="/css/font.css" rel="stylesheet" type="text/css">
  <link href="/css/kube.css" rel="stylesheet" type="text/css">
  <link href="/css/highlight.css" rel="stylesheet" type="text/css">
  <link href="/css/master.css" rel="stylesheet" type="text/css">
  
 <link href="/css/custom.css" rel="stylesheet" type="text/css">
  
  <script src="/js/jquery-2.1.4.min.js" type="text/javascript">
  </script>

  <script type="text/javascript" src="/js/tocbot.min.js"></script>

  
  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
  
</head>

<body class="page-kube">
  <header> <div class="show-sm">
    <div id="nav-toggle-box">
      <div id="nav-toggle-brand">
        <a href="/">Escape Velocity</a>
      </div><a data-component="toggleme" data-target="#top" href="#" id="nav-toggle"><i class="kube-menu"></i></a>
    </div>
  </div>
  <div class="hide-sm" id="top">
    <div id="top-brand">
      <a href="/" title="home">Escape Velocity</a>
    </div>
    <nav id="top-nav-main">
      <ul>
       
       <li><a href="/notes">Notes</a></li>
       <li><a href="/blog">Blog</a></li>
       <li><a href="/about">About</a></li>
      </ul>
    </nav>
    <nav id="top-nav-extra">
      <ul>
        
      </ul>
    </nav>
  </div>
 </header>
  <main>
  <div id="main">
    <div id="hero">
      <h1> Matrizen </h1>
      <p class="hero-lead">
         Matrizen, deren Eigenschaften und Interpretationen
      </p>

    </div>
    <div id="kube-component" class="content">
    
<nav id="contents">
    <ol class="js-toc">
    </ol>
</nav>
<script type="text/javascript">
document.addEventListener("DOMContentLoaded",
function(){
tocbot.init({

tocSelector: '.js-toc',

contentSelector: '.content',

headingSelector: 'h1'
})
}
);
</script>




    <p>Eine 
<span>
                  
        \(m\times n\)
    
</span>
-Matrix über einem Körper 
<span>
                  
        \(\mathbb{K}\)
    
</span>
 ist eine Anordnung von Element von 
<span>
                  
        \(\mathbb{K}\)
    
</span>
 nach folgendem Schema (mit 
<span>
                  
        \(m\)
    
</span>
 Zeilen und 
<span>
                  
        \(n\)
    
</span>
 Spalten):

<span>
     
        $$\begin{pmatrix} a_{11} &amp; \ldots &amp; a_{1n} \\ \vdots &amp; \cdots &amp; \vdots \\ a_{m1} &amp; \ldots &amp; a_{mn} \end{pmatrix}$$
    
</span>
</p>
<p>Die Menge aller solcher Matrizen wird mit 
<span>
                  
        \(M_{mn}(\mathbb{K})\)
    
</span>
 bezeichnet.</p>
<p>Matrizen können auch über einem kommutativen Ring statt einem Körper definiert werden. Dann ergeben sich folgende Unterschiede:</p>
<ul>
<li>Matrizen über kommutativen Ringen können nicht notwendigerweise in Normalform überführt werden.</li>
</ul>
<p>Der <strong>Rang</strong> einer Matrix ist die Anzahl linear unabhängiger Spalten. Oder Zeilen, denn Spalten- und Zeilenrang sind immer gleich. (Die möglichen Ränge einer 
<span>
                  
        \(m\times n\)
    
</span>
-Matrix sind also 
<span>
                  
        \(0\)
    
</span>
 bis 
<span>
                  
        \(\text{min}(m,n)\)
    
</span>
.)</p>
<p>Die <strong>Spur</strong> einer Matrix ist die Summe der Diagonalelemente.</p>
<h1 id="matrizenrechnung">Matrizenrechnung</h1>
<p>Addition und Skalarmultiplikation passieren elementweise.</p>
<p>Die Matrizenmultiplikation ist eine Verknüpfung 
<span>
                  
        \(M_{mk}\times M_{kn}\to M_{mn}\)
    
</span>
, wobei sich 
<span>
                  
        \(C=AB\)
    
</span>
 so berechnet, dass der Eintrag i,j in 
<span>
                  
        \(C\)
    
</span>
 das Standardskalarprodukt der i-ten Zeile von 
<span>
                  
        \(A\)
    
</span>
 mit der j-ten Spalte von 
<span>
                  
        \(B\)
    
</span>
 ist:

<span>
                  
        \(c_{ij}=\sum_{k=1}^m a_{ik}b_{kj}\)
    
</span>
.</p>
<p>Zum Beispiel 
<span>
                  
        \(c_{11}=a_{11}b_{11}&#43;a_{12}b_{21}&#43;a_{13}b_{31}\)
    
</span>
.</p>
<p><img src="/images/docs/matrixmultiplication.png" alt="Matrizenmultiplikation"></p>
<p>Eine Matrix ist <strong>nilpotent</strong>, wenn es ein 
<span>
                  
        \(m\in\mathbb{N}\)
    
</span>
 gibt, so dass 
<span>
                  
        \(A^m = 0\)
    
</span>
 und 
<span>
                  
        \(A^{m-1}\neq 0\)
    
</span>
. Für nilpotente Matrizen gilt:</p>
<ul>
<li>Das <a href="#das-charakteristische-polynom-einer-matrix">charakteristische Polynom</a> ist von der Form 
<span>
                  
        \(x^n\)
    
</span>
. Die einzige Nullstelle davon ist 0, also haben nilpotente Matrizen nur den Eigenwert 0.</li>
<li>Da sie den Eigenwert 0 haben, ist ihr Kern nicht trivial und damit sind sie nicht <a href="#invertierbarkeit">invertierbar</a>.</li>
<li>Außerdem sind ihre Determinante und Spur jeweils 0.</li>
<li>Sie sind entweder die Nullmatrix oder nicht diagonalisierbar.</li>
<li>Sie haben keinen vollen Rang, d.h. ihre Spaltenvektoren sind linear abhängig.</li>
</ul>
<h2 id="elementarmatrizen">Elementarmatrizen</h2>
<p>Die Identitätsmatrix 
<span>
                  
        \(I_n\)
    
</span>
 ist die 
<span>
                  
        \(n\times n\)
    
</span>
-Matrix, die in der Hauptdiagonale den Wert 1 hat und sonst 0. Zum Beispiel:

<span>
     
        $$I_1 = (1) \quad I_2 = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix} \quad I_3 = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix}$$
    
</span>
</p>
<p>Elementarmatrizen unterscheiden sich von der Identitätsmatrix nur durch die Anwendung einer
elementaren Zeilenumformung:</p>
<ul>
<li>
<span>
                  
        \(P_{ij}\)
    
</span>
 Vertauschen der Zeilen 
<span>
                  
        \(i\)
    
</span>
 und 
<span>
                  
        \(j\)
    
</span>
</li>
<li>
<span>
                  
        \(D_{i}(c)\)
    
</span>
 Multiplikation der Zeile 
<span>
                  
        \(i\)
    
</span>
 mit einem Skalar 
<span>
                  
        \(c\)
    
</span>
</li>
<li>
<span>
                  
        \(T_{ij}(c)\)
    
</span>
 Addition des 
<span>
                  
        \(c\)
    
</span>
-fachen der Zeile 
<span>
                  
        \(j\)
    
</span>
 zu einer anderen (nicht derselben!) Zeile 
<span>
                  
        \(i\)
    
</span>
</li>
</ul>
<p>Analog für Spalten. Diese Umformungen ändern den Rang einer Matrix nicht.</p>
<h2 id="transponierte-inverse-und-adjungierte-matrix">Transponierte, inverse und adjungierte Matrix</h2>
<p>Die <strong>Transponierte</strong> 
<span>
                  
        \(A^T\)
    
</span>
 einer Matrix 
<span>
                  
        \(A\)
    
</span>
 ist diejenige Matrix, in der Zeilen und Spalten vertauscht sind.</p>
<p>Die <strong>Inverse</strong> 
<span>
                  
        \(A^{-1}\)
    
</span>
 einer Matrix 
<span>
                  
        \(A\)
    
</span>
 ist diejenige Matrix, so dass 
<span>
                  
        \(A\cdot A^{-1} = A^{-1}\cdot A = I\)
    
</span>
. Siehe <a href="#invertierbarkeit">Invertierbarkeit</a>.</p>
<p>Die <strong>adjungierte Matrix</strong> einer komplexen Matrix 
<span>
                  
        \(A\)
    
</span>
 ist 
<span>
                  
        \(A^H=\overline{A}^T = \overline{A^T}\)
    
</span>
.</p>
<p>Es gelten folgende Eigenschaften:</p>
<table>
<thead>
<tr>
<th>Transponierte</th>
<th>Adjungierte</th>
<th>Inverse</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<span>
                  
        \((A&#43;B)^T = A^T &#43; B^T\)
    
</span>
</td>
<td>
<span>
                  
        \((A&#43;B)^H = A^H &#43; B^H\)
    
</span>
</td>
<td></td>
</tr>
<tr>
<td>
<span>
                  
        \((A\cdot B)^T = B^T \cdot A^T\)
    
</span>
</td>
<td>
<span>
                  
        \((A\cdot B)^H = B^H \cdot A^H\)
    
</span>
</td>
<td>
<span>
                  
        \((A\cdot B)^{-1} = B^{-1} \cdot A^{-1}\)
    
</span>
</td>
</tr>
<tr>
<td>
<span>
                  
        \((cA)^T = cA^T\)
    
</span>
</td>
<td>
<span>
                  
        \((cA)^H = \overline{c}A^H\)
    
</span>
</td>
<td>
<span>
                  
        \((cA)^{-1} = c^{-1}A^{-1}\)
    
</span>
</td>
</tr>
<tr>
<td>
<span>
                  
        \((A^T)^T = A\)
    
</span>
</td>
<td>
<span>
                  
        \((A^H)^H = A\)
    
</span>
</td>
<td>
<span>
                  
        \((A^{-1})^{-1} = A\)
    
</span>
</td>
</tr>
<tr>
<td>
<span>
                  
        \((A^{-1})^T = (A^T)^{-1}\)
    
</span>
</td>
<td>
<span>
                  
        \((A^{-1})^H = (A^H)^{-1}\)
    
</span>
</td>
<td></td>
</tr>
<tr>
<td>
<span>
                  
        \(\overline{A^T} = (\overline{A})^T\)
    
</span>
</td>
<td></td>
<td>
<span>
                  
        \(\overline{A^{-1}} = (\overline{A})^{-1}\)
    
</span>
</td>
</tr>
<tr>
<td>
<span>
                  
        \(\text{rang}({A^T}) = \text{rang}({A})\)
    
</span>
</td>
<td>
<span>
                  
        \(\text{rang}({A^H}) = \text{rang}({A})\)
    
</span>
</td>
<td>
<span>
                  
        \(\text{rang}({A^{-1}}) = \text{rang}({A})\)
    
</span>
</td>
</tr>
<tr>
<td>
<span>
                  
        \(\text{spur}({A^T}) = \text{spur}({A})\)
    
</span>
</td>
<td>
<span>
                  
        \(\text{spur}({A^H}) = \overline{\text{spur}({A})}\)
    
</span>
</td>
<td></td>
</tr>
<tr>
<td>
<span>
                  
        \(\text{det}({A^T}) = \text{det}({A})\)
    
</span>
</td>
<td>
<span>
                  
        \(\text{det}({A^H}) = \overline{\text{det}({A})}\)
    
</span>
</td>
<td>
<span>
                  
        \(\text{det}({A^{-1}}) = (\text{det}({A}))^{-1}\)
    
</span>
</td>
</tr>
</tbody>
</table>
<p>Eine reelle Matrix heißt</p>
<ul>
<li><strong>symmetrisch</strong>, wenn 
<span>
                  
        \(A^T=A\)
    
</span>
 (bzw. wenn 
<span>
                  
        \(a_{ij}=a_{ji}\)
    
</span>
 für alle Einträge), und schiefsymmetrisch, wenn 
<span>
                  
        \(A^T=-A\)
    
</span>
</li>
<li><strong>orthogonal</strong>, wenn 
<span>
                  
        \(A^T=A^{-1}\)
    
</span>
 (und damit 
<span>
                  
        \(A^T\cdot A = I\)
    
</span>
).</li>
</ul>
<p>Eine komplexe Matrix heißt</p>
<ul>
<li><strong>hermitesch</strong>, wenn 
<span>
                  
        \(A^T=\overline{A}\)
    
</span>
 (und damit 
<span>
                  
        \(A^H=A\)
    
</span>
), und schiefhermitesch, wenn 
<span>
                  
        \(A^T=-\overline{A}\)
    
</span>
</li>
<li><strong>unitär</strong>, wenn 
<span>
                  
        \(A^H=A^{-1}\)
    
</span>
 (und damit 
<span>
                  
        \(A^H\cdot A = I\)
    
</span>
).</li>
</ul>
<p>Reelle symmetrische Matrizen und komplexe hermitesche Matrizen haben viele Eigenschaften gemeinsam.</p>
<h1 id="determinante">Determinante</h1>
<p>Die <strong>Determinante</strong> ist eine eindeutige Abbildung 
<span>
                  
        \(M_{nn}(\mathbb{K})\to\mathbb{K}\)
    
</span>
, die so definiert ist, dass sie genau dann 0 wird, wenn die Spalten der Matrix nicht linear unabhängig sind (die Matrix also nicht invertierbar ist):

<span>
     
        $$\text{det}(A) = \sum_{\sigma\in S_n}\text{sgn}(\sigma)a_{1\sigma(1)}\cdots a_{n\sigma(n)}$$
    
</span>
</p>
<p>Für die Berechnung der Determinante ist die Leibniz-Formel aber nur bis 
<span>
                  
        \(n=2\)
    
</span>
 praktisch handhabbar.</p>
<h2 id="für-hahahugoshortcode-s77-hbhb-trivial">Für 
<span>
                  
        \(n=1\)
    
</span>
: trivial</h2>

<span>
     
        $$\text{det}\begin{pmatrix} a \end{pmatrix} = a$$
    
</span>

<h2 id="für-hahahugoshortcode-s79-hbhb-einfach">Für 
<span>
                  
        \(n=2\)
    
</span>
: einfach</h2>
<p>
<span>
     
        $$\text{det}\begin{pmatrix} a_{11} &amp; a_{12} \\ a_{21} &amp; a_{22} \end{pmatrix} = a_{11} a_{22} - a_{12} a_{21}$$
    
</span>

Das lässt sich gut nachvollziehen, denn wenn die Spaltenvektoren linear abhängig sind (und von 0 verschieden), heißt das es gibt ein 
<span>
                  
        \(c\in\mathbb{K}\)
    
</span>
, so dass:

<span>
     
        $$\begin{pmatrix} a_{11} \\ a_{12} \end{pmatrix} = c\cdot \begin{pmatrix} a_{21} \\ a_{22} \end{pmatrix}$$
    
</span>

Also 
<span>
                  
        \(a_{11} = c\cdot a_{21}\)
    
</span>
 und 
<span>
                  
        \(a_{12} = c\cdot a_{22}\)
    
</span>
,
d.h. 
<span>
                  
        \(c=\dfrac{a_{11}}{a_{21}}\)
    
</span>
 und damit 
<span>
                  
        \(a_{12} = \dfrac{a_{11}}{a_{21}} a_{22}\)
    
</span>
. Daraus ergibt sich 
<span>
                  
        \(a_{12} a_{21} = a_{11} a_{22}\)
    
</span>
 bzw. 
<span>
                  
        \(0 = a_{11} a_{22} - a_{12} a_{21}\)
    
</span>
.</p>
<h2 id="für-hahahugoshortcode-s89-hbhb-sarrus-regel">Für 
<span>
                  
        \(n=3\)
    
</span>
: Sarrus-Regel</h2>
<p><img src="../img/sarrus.png" alt="Saruss-Regel"></p>
<p>Man summiert:</p>
<ul>
<li>durchgezogene Linie: positives Produkt der Elemente</li>
<li>gestrichelte Linie: negatives Produkt der Elemente</li>
</ul>
<p>Funktioniert genauso, wenn man die ersten beiden Zeilen unter die Matriz schreibt.</p>
<h2 id="für-hahahugoshortcode-s90-hbhb-laplace-entwicklung">Für 
<span>
                  
        \(n&gt;3\)
    
</span>
: Laplace-Entwicklung</h2>
<p>Die Laplace-Entwicklung nach der i-ten Zeile ist die Summe aus folgendem Summanden für jedes Element 
<span>
                  
        \(a_{ij}\)
    
</span>
:

<span>
     
        $$a_{ij}\cdot(-1)^{i&#43;j}\cdot\text{det}(A_{ij})$$
    
</span>

Wobei 
<span>
                  
        \(A_{ij}\)
    
</span>
 die Matrix ist, die übrig bleibt, wenn man in 
<span>
                  
        \(A\)
    
</span>
 die i-te Zeile und j-te Spalte (also jeweils die Zeile und Spalte mit dem Eintrag 
<span>
                  
        \(a_{ij}\)
    
</span>
) entfernt.</p>
<p>Je mehr 0en in einer Zeile stehen, desto einfacher wird die Laplace-Entwicklung, deswegen macht es manchmal Sinn, erst elementare Zeilenumformungen auf die Matrix anzuwenden (wodurch sich die Determinante höchstens im Vorzeichen ändert, siehe <em>Eigenschaften</em> unten). Die Matrix wird also um 
<span>
                  
        \(n=1\)
    
</span>
 kleiner. Das wiederholt man so lange, bis 
<span>
                  
        \(n&lt;3\)
    
</span>
.</p>
<p>Analog funktioniert die Laplace-Entwicklung nach der j-ten Spalte.</p>
<h2 id="spezialfälle">Spezialfälle</h2>
<ul>
<li>
<p>Wenn eine Zeile oder Spalte 0 ist, ist die Determinante 0.</p>
</li>
<li>
<p>Wenn eine Zeile oder Spalte doppelt vorkommt, ist die Determinante 0.</p>
</li>
<li>
<p>
<span>
                  
        \(\text{det}(\text{normalform}(A))\)
    
</span>
 = Produkt der Diagonalelemente</p>
<p>Die Determinante wird also genau dann 0, wenn das Produkt der Diagonalelemente der Normalform 0 ist, d.h. wenn mindestens eins der Diagonalelemente 0 ist. Die Determinanten erfasst damit, ob eine quadratische Matrix invertierbar (regulär) ist. Normalform bedeutet obere Dreiecksmatrix.</p>
</li>
</ul>
<h2 id="eigenschaften">Eigenschaften</h2>
<p>Die Determinante hat die folgenden Eigenschaften:</p>
<ul>
<li>
<p>
<span>
                  
        \(\text{det}(I)=1\)
    
</span>
 (Identitätsmatrix)</p>
</li>
<li>
<p>
<span>
                  
        \(\text{det}(cA)=c^n\text{det}(A)\)
    
</span>
 (Skalarfaktor)</p>
</li>
<li>
<p>
<span>
                  
        \(\text{det}(AB)=\text{det}(A)\,\text{det}(B)\)
    
</span>
 für Matrizen über Integritätsbereichen</p>
</li>
<li>
<p>
<span>
                  
        \(\text{det}(A)=0\)
    
</span>
 genau dann, wenn 
<span>
                  
        \(\text{rang}(A) &lt; n\)
    
</span>
</p>
<p>Ob 
<span>
                  
        \(\text{det}(A)=0\)
    
</span>
 oder nicht ändert sich also nicht durch Zeilenumformungen. Die Determinanten ist damit eine Invariante der Matrix.</p>
<p>Für Elementarmatrizen gilt:</p>
<ul>
<li>
<span>
                  
        \(\text{det}(P_{ij}) = -1\)
    
</span>
, also

<span>
                  
        \(\text{det}(P_{ij}A)=-\text{det}(A)\)
    
</span>
</li>
<li>
<span>
                  
        \(\text{det}(D_{i}(c)) = c\)
    
</span>
, also

<span>
                  
        \(\text{det}(D_{i}(c)A)=c\cdot\text{det}(A)\)
    
</span>
</li>
<li>
<span>
                  
        \(\text{det}(T_{ij}(c)) = 1\)
    
</span>
, also

<span>
                  
        \(\text{det}(T_{ij}(c)A)=\text{det}(A)\)
    
</span>
</li>
</ul>
</li>
</ul>
<h1 id="invertierbarkeit">Invertierbarkeit</h1>
<p>Für eine 
<span>
                  
        \(n\times n\)
    
</span>
 Matrix 
<span>
                  
        \(A\)
    
</span>
 sind die folgenden Aussagen äquivalent:</p>
<ul>
<li>
<span>
                  
        \(A\)
    
</span>
 ist invertierbar.</li>
<li>
<span>
                  
        \(\text{rang}(A)=n\)
    
</span>
</li>
<li>Die Spaltenvektoren von 
<span>
                  
        \(A\)
    
</span>
 sind linear unabhängig.</li>
<li>
<span>
                  
        \(A\)
    
</span>
 kann als endliches Produkt von Elementarmatrizen ausgedrückt werden.</li>
<li>
<span>
                  
        \(\text{det}(A)\neq 0\)
    
</span>
 (d.h. wenn 
<span>
                  
        \(\text{det}(A)\)
    
</span>
 im Körper oder Ring, über dem die Matrix definiert ist, invertierbar ist)</li>
<li>0 ist kein Eigenvektor von 
<span>
                  
        \(A\)
    
</span>
 bzw. 
<span>
                  
        \(\text{ker}(A)\neq\{0\}\)
    
</span>
.</li>
</ul>
<p><strong>Berechnung der Inversen:</strong></p>
<p>Gilt 
<span>
                  
        \(AB=C\)
    
</span>
 und wendet man die gleichen Zeilenumformungen auf 
<span>
                  
        \(A\)
    
</span>
 und 
<span>
                  
        \(C\)
    
</span>
 an (mit dem Ergebnis 
<span>
                  
        \(A&#39;\)
    
</span>
 und 
<span>
                  
        \(C&#39;\)
    
</span>
), so ist 
<span>
                  
        \(A&#39;B=C&#39;\)
    
</span>
. Da 
<span>
                  
        \(AA^{-1}=I\)
    
</span>
, bedeutet das, dass man die Inverse einer Matrix bestimmen kann, indem man die gleichen Zeilenumformungen, die 
<span>
                  
        \(A\)
    
</span>
 in 
<span>
                  
        \(I\)
    
</span>
 überführen, ausführen kann, um 
<span>
                  
        \(I\)
    
</span>
 in 
<span>
                  
        \(A^{-1}\)
    
</span>
 zu überführen.</p>
<h1 id="das-charakteristische-polynom-einer-matrix">Das charakteristische Polynom einer Matrix</h1>
<p>Für Matrizen 
<span>
                  
        \(A\in M_{nn}(\mathbb{K})\)
    
</span>
 berecht man das charakteristische Polynom wie folgt:</p>

<span>
     
        $$\chi_A=\text{det}(xI_n-A)$$
    
</span>

<p>Die Nullstellen des charakteristischen Polynoms sind die Eigenwerte der Matrix.</p>
<p>Wenn 
<span>
                  
        \(A\)
    
</span>
 die Nullmatrix oder nilpotent ist, ist 
<span>
                  
        \(\chi_A=x^n\)
    
</span>
 (und umgekehrt).</p>
<h1 id="eigenwerte-eigenvektoren-eigenraum">Eigenwerte, Eigenvektoren, Eigenraum</h1>
<p>Die <strong>Eigenvektoren</strong> 
<span>
                  
        \(v\)
    
</span>
 eines Endomorphismus 
<span>
                  
        \(f\)
    
</span>
 eines Vektorraums 
<span>
                  
        \(V\)
    
</span>
 bzw. seiner Matrixdarstellung 
<span>
                  
        \(A\)
    
</span>
 sind alle (vom Nullvektor verschiedene) Vektoren, die durch die Transformation nur gestaucht oder gestreckt werden, deren Richtung aber gleich bleibt. Der dazugehörige <strong>Eigenwert</strong> 
<span>
                  
        \(\lambda\)
    
</span>
 ist der Faktor der Stauchung oder Streckung.
Formal:

<span>
     
        $$\quad f(v) = \lambda v \quad\text{bzw.}\quad Av = \lambda v$$
    
</span>
</p>
<p>D.h. 
<span>
                  
        \(f(v)\)
    
</span>
 bzw. 
<span>
                  
        \(Av\)
    
</span>
 hat die gleiche Richtung wie 
<span>
                  
        \(v\)
    
</span>
, nur um den Faktor 
<span>
                  
        \(\lambda\)
    
</span>
 gestaucht oder gestreckt.</p>
<ul>
<li>Eine Matrix muss keine Eigenvektoren besitzen - eine Rotation z.B. ändert die Richtung aller Vektoren im Raum (und 
<span>
                  
        \(\lambda\)
    
</span>
 entspricht dann der Rotationsachse).</li>
<li>
<span>
                  
        \(A\)
    
</span>
 und 
<span>
                  
        \(A^T\)
    
</span>
 haben möglicherweise unterschiedliche Eigenwerte.</li>
<li>Sind die Eigenwerte verschieden, sind die dazugehörigen Eigenvektoren linear unabhängig.</li>
</ul>
<p>Eigenwerte sind die Nullstellen des charakteristischen Polynoms.
Eine Matrix 
<span>
                  
        \(A\in M_{nn}\)
    
</span>
 kann also höchstens 
<span>
                  
        \(n\)
    
</span>
 Eigenwerte haben.
Wie oft eine Nullstelle vorkommt, nennt man die <strong>algebraische Vielfachheit</strong> des Eigenwerts.</p>
<p>Die Eigenvektoren zu einem Eigenwert spannen zusammen mit dem Nullvektor einen Unterraum auf, den <strong>Eigenraum</strong>:

<span>
     
        $$\begin{aligned}\text{Eigenraum}(f,\lambda) &amp;= \{0\}\cup\{ v\in V \,|\, v\text{ ist ein Eigenvektor von }f \} \\ &amp;= \{ v\in V \,|\, f(v) = \lambda v \}\\ &amp;= \text{ker}(A-\lambda I_n) \end{aligned}$$
    
</span>

Die Dimension des Eigenraums ist die <strong>geometrische Vielfachheit</strong> des Eigenwerts.</p>
<p><strong>Berechnung:</strong></p>
<ol>
<li>Um die Eigenwerte 
<span>
                  
        \(\lambda\)
    
</span>
 zu bestimmen, berechnet man die Nullstellen des charakteristischen Polynoms.</li>
<li>Die Gleichung 
<span>
                  
        \(Av = \lambda v\)
    
</span>
 lässt sich äquivalent umformen zu:

<span>
     
        $$(A-\lambda I_n)v=0$$
    
</span>

Setzt man die Eigenwerte 
<span>
                  
        \(\lambda\)
    
</span>
 in diese Gleichung ein, erhält man ein Gleichungssystem, über das sich jeweils die zugehörigen Eigenvektoren 
<span>
                  
        \(v\)
    
</span>
 bestimmen lassen. (Die müssen nicht eindeutig sein.)</li>
<li>Überprüfen kann man die gefundenen Eigenvektoren dann durch Einsetzen in 
<span>
                  
        \(Av = \lambda v\)
    
</span>
.</li>
</ol>
<h1 id="normalformen">Normalformen</h1>
<p>Matrizen lassen sich nach <strong>Ähnlichkeit</strong> in Äquivalenzklassen einteilen, wobei eine Klasse alle Matrizen enthält, die den gleichen Endomorphismus darstellen.
Technisch heißt das:</p>
<p>Zwei Matrizen 
<span>
                  
        \(A,B\)
    
</span>
 sind <strong>ähnlich</strong>, wenn es eine invertierbare Matrix 
<span>
                  
        \(S\)
    
</span>
 gibt, so dass:

<span>
     
        $$A=S^{-1}BS$$
    
</span>

Das bedeutet, dass 
<span>
                  
        \(A\)
    
</span>
 die gleiche Transformation wie 
<span>
                  
        \(B\)
    
</span>
 ausdrückt, nur in einer anderen Basis, wobei 
<span>
                  
        \(S\)
    
</span>
 die Basiswechselmatrix ist (
<span>
                  
        \(S\)
    
</span>
 ist nicht eindeutig, denn jedes Vielfache 
<span>
                  
        \(cS\)
    
</span>
 erfüllt die Gleichung auch). D.h. 
<span>
                  
        \(A\)
    
</span>
 und 
<span>
                  
        \(B\)
    
</span>
 stellen die gleiche Transformation dar, nur in unterschiedlichen Koordinatensystemen.
Die Darstellungsmatrix eines Endomorphismus lässt sich also durch geschickte Wahl der Basis (und einen entsprechenden Basiswechsel) in eine Normalform bringen.</p>
<p>Ähnliche Matrizen haben:</p>
<ul>
<li>den gleichen Rang,</li>
<li>die gleiche Determinante,</li>
<li>das gleiche charakteristische Polynom und Minimalpolynom,</li>
<li>die gleichen Eigenwerte (aber nicht notwendigerweise die gleichen Eigenvektoren),</li>
<li>die gleiche Jordan-Normalform.</li>
</ul>
<p>Diese Punkte sind alle notwendig, hinreichend ist aber nur der letzte. Das heißt:</p>
<ul>
<li>Wenn zwei Matrizen unterschiedliche Ränge, Determinanten, charakteristische Polynome oder Eigenwerte haben, sind sie nicht ähnlich.</li>
<li>Wenn zwei Matrizen die gleiche Jordan-Normalform haben, sind sie ähnlich.</li>
</ul>
<p>Matrizen lassen sich auch nach <strong>Kongruenz</strong> in Äquivalenzklassen (sogenannte <strong>Kongruenzklassen</strong>) einteilen:
Zwei Matrizen 
<span>
                  
        \(A,B\)
    
</span>
 sind <strong>kongruent</strong>, wenn es eine invertierbare Matrix 
<span>
                  
        \(S\)
    
</span>
 gibt, so dass

<span>
     
        $$A=S^TBS$$
    
</span>
</p>
<p>Da in der Regel 
<span>
                  
        \(S^T\neq S^{-1}\)
    
</span>
, sind kongruente Matrizen in der Regel nicht ähnlich.</p>
<h2 id="treppennormalform">Treppennormalform</h2>
<ul>
<li>Alle Nullreihen stehen ganz unten.</li>
<li>In jeder Zeile ist der von links erste Eintrag ungleich 0 eine 1. Das sind die Pivot-Positionen.</li>
<li>Stufung: Jede Pivot-Position ist rechts von der Pivot-Position der Zeile darüber.</li>
<li>Daraus folgt, dass alle Einträge unter den Pivot-Positionen 0 sind. In der reduzierten Treppennormalform sind auch alle Einträge über den Pivot-Positionen 0.</li>
</ul>
<p>Die Treppennormalform erleichtert das Lösen eines linearen Gleichungssystems.</p>
<p><strong>Beispiele:</strong>

<span>
     
        $$\left(\begin{matrix} 1 &amp; \ast &amp; \ast &amp; \ast \\ 0 &amp; 0 &amp; 1 &amp; \ast \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{matrix}\right) \qquad \left(\begin{matrix} 1 &amp; \ast &amp; \ast \\ 0 &amp; 1 &amp; \ast \\ 0 &amp; 0 &amp; 0 \end{matrix}\right) \qquad \left(\begin{matrix} 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{matrix}\right)$$
    
</span>
</p>
<p><strong>Berechnung:</strong> Gaußsches Eliminationsverfahren</p>
<h2 id="diagonalisierbarkeit">Diagonalisierbarkeit</h2>
<p>Eine Matrix 
<span>
                  
        \(A\)
    
</span>
 ist diagonalisierbar, wenn sie zu einer Diagonalmatrix ähnlich ist (d.h. zu einer Matrix, in der alle Einträge, die nicht Diagonalelemente sind, 0 sind), d.h. wenn es eine invertierbare Matrix 
<span>
                  
        \(S\)
    
</span>
 gibt, so dass 
<span>
                  
        \(S^{-1}AS\)
    
</span>
 eine Diagonalmatrix ist. Die Eigenwerte der Matrix sind dann die Diagonalelemente von 
<span>
                  
        \(S^{-1}AS\)
    
</span>
 und 
<span>
                  
        \(S\)
    
</span>
 hat als Spalten die zugehörigen Eigenvektoren.</p>
<p>Eine Matrix auf jeden Fall diagonalisierbar, wenn</p>
<ul>
<li>sie symmetrisch ist;</li>
<li>sie die maximale Anzahl Eigenwerte hat (= Anzahl der Dimensionen des Vektorraums), denn dann sind die zugehörigen Eigenvektoren linear unabhängig und es gibt eine Basis aus Eigenvektoren, d.h. der Vektorraum ist die Summe der Eigenräume;</li>
<li>das charakteristische Polynom in Linearfaktoren zerfällt und die algebraische und geometrische Vielfachheit der Eigenwerte übereinstimmen.</li>
</ul>
<h2 id="jordan-normalform">Jordan-Normalform</h2>
<p>Ist eine Matrix nicht diagonalisierbar, will man einer Diagonalform möglichst nahe kommen. Das kann man mit der Jordan-Normalform, in die jede Matrix überführt werden kann.</p>
<p>Eine Jordan-Matrix enthälten auf der Diagonalen Jordan-Blöcke und sonst 0. Ein Jordan-Block ist eine quadratische Matrix mit einem Eigenwert auf der Diagonalen, 1 auf einer der Nebendiagonalen und sonst 0. Zum Beispiel:</p>
<ul>
<li>
<span>
                  
        \(\,\,(\lambda_1)\)
    
</span>
</li>
<li>
<span>
                  
        \(\,\begin{pmatrix} \lambda_2 &amp; 1 \\ 0 &amp; \lambda_2 \end{pmatrix}\text{ oder }\begin{pmatrix} \lambda_2 &amp; 0 \\ 1 &amp; \lambda_2 \end{pmatrix}\)
    
</span>
</li>
<li>
<span>
                  
        \(\begin{pmatrix} \lambda_3 &amp; 1 &amp; 0 \\ 0 &amp; \lambda_3 &amp; 1 \\ 0 &amp; 0 &amp; \lambda_3 \end{pmatrix}\text{ oder }\begin{pmatrix} \lambda_3 &amp; 0 &amp; 0 \\ 1 &amp; \lambda_3 &amp; 0 \\ 0 &amp; 1 &amp; \lambda_3 \end{pmatrix}\)
    
</span>
</li>
</ul>
<p>Die Reihenfolge der Blöcke in einer Jordan-Matrix ist egal.</p>
<p><strong>Berechnung:</strong> Für eine Matrix 
<span>
                  
        \(A\in M_{nn}(\mathbb{K})\)
    
</span>
.</p>
<ol>
<li>Eigenwerte der Matrix berechnen, d.h. die Nullstellen des charakteristischen Polynoms 
<span>
                  
        \(\Chi_A=\text{det}(xI_n-A)\)
    
</span>
 bestimmen, zusammen mit ihrer algebraischen Vielfachheit. (Wenn 
<span>
                  
        \(\Chi_A\)
    
</span>
 nicht in Linearfaktoren über 
<span>
                  
        \(\mathbb{K}\)
    
</span>
 zerfällt, dann hat die Matrix keine Jordan-Normalform in 
<span>
                  
        \(\mathbb{K}\)
    
</span>
.)</li>
<li>Für jeden Eigenwert 
<span>
                  
        \(\lambda\)
    
</span>
:
<ul>
<li>Wenn die algebraische Vielfachheit 1 ist, dann gibt es einen Jordan-Block der Größe 1, also 
<span>
                  
        \((\lambda)\)
    
</span>
.</li>
<li>Ansonsten berechne die Haupträume 
<span>
                  
        \(H_k = \text{ker}((A-\lambda I_n)^k)\)
    
</span>
 zu 
<span>
                  
        \(\lambda\)
    
</span>
, wobei

<span>
     
        $$\{0\} \subset H_1 \subset H_2 \subset \ldots$$
    
</span>

bis 
<span>
                  
        \(\text{dim}(H_k)\)
    
</span>
 die algebraische Vielfachheit von 
<span>
                  
        \(\lambda\)
    
</span>
 ist. Dann wissen wir:
Es gibt 
<span>
                  
        \(\text{dim}(H_1)\)
    
</span>
 viele Jordan-Blöcke für 
<span>
                  
        \((\lambda)\)
    
</span>
 und
davon sind 
<span>
                  
        \(\text{dim}(H_{i&#43;1}) - \text{dim}(H_i)\)
    
</span>
 viele Jordan-Blöcke von mindestens der Größe 
<span>
                  
        \(i\)
    
</span>
.</li>
</ul>
</li>
<li>Aus diesen Informationen können wir die Jordan-Normalform bauen.</li>
</ol>
<h1 id="matrizen-als-lineare-transformationen">Matrizen als lineare Transformationen</h1>
<p>Eine Matrix 
<span>
                  
        \(A\in M_{mn}(\mathbb{K})\)
    
</span>
 kann eine lineare Abbildung 
<span>
                  
        \(f:\mathbb{K}^n\to\mathbb{K}^m\)
    
</span>
 zwischen endlichen Vektorräumen darstellen. Sie bildet dann die Vektoren 
<span>
                  
        \(x\in\mathbb{K}^n\)
    
</span>
 auf die Vektoren 
<span>
                  
        \(Ax\in\mathbb{K}^m\)
    
</span>
 ab, d.h. 
<span>
                  
        \(f(x)=Ax\)
    
</span>
. (Dabei können diese Vektoren zum Beispiel auch die Koordinatenvektoren beliebiger Vektoren aus 
<span>
                  
        \(\mathbb{K}\)
    
</span>
 sein.)</p>
<p>Linear ist eine solche Abbildung, weil die Matrizenmultiplikation sowohl Addition als auch Skalarmultiplikation respektiert:</p>
<ul>
<li>
<span>
                  
        \(A(x&#43;y) = Ax &#43; Ay\)
    
</span>
</li>
<li>
<span>
                  
        \(A(cx) = c(Ax)\)
    
</span>
</li>
</ul>
<p>In einem zweidimensionalen Raum kann man Linearität anschaulich so verstehen, dass die Transformation des Raumes seine Gridlinien parallel lässt und der Abstand zwischen ihnen überall gleich bleibt.</p>
<p>Da jeder Vektor als Linearkombination der kanonischen Basisvektoren dargestellt werden kann, ist eine lineare Abbildung zwischen Vektorräumen vollständig dadurch bestimmt, worauf diese Basisvektoren abgebildet werden.
Eine Matrix als Darstellung einer linearen Abbildung enthält als Spalten nun genau die Vektoren, auf die die Basisvektoren abgebildet werden.
Nehmen wir die kanonischen Basisvektoren von 
<span>
                  
        \(x\in\mathbb{K}^n\)
    
</span>
:

<span>
     
        $$e_1 = \begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{pmatrix}, \ldots, e_n = \begin{pmatrix} 0 \\ \vdots \\ 0 \\ 1 \end{pmatrix}$$
    
</span>
</p>
<p>Multiplikation einer Matrix 
<span>
                  
        \(A\)
    
</span>
 mit einem Basisvektor 
<span>
                  
        \(e_i\)
    
</span>
 ergibt genau die i-te Spalte der Matrix, d.h. 
<span>
                  
        \(Ae_i\)
    
</span>
 ist der Vektor, auf den 
<span>
                  
        \(e_i\)
    
</span>
 abgebildet wird.</p>
<p>Allgemeiner bedeutet die Multiplikation der Matrix 
<span>
                  
        \(A\)
    
</span>
 mit einem beliebigen Vektor 
<span>
                  
        \(x\)
    
</span>
 die Anwendung der Transformation auf diesen Vektor.
Denn jeder Vektor kann als Linearkombination von Basisvektoren geschrieben werden kann, aufgrund der Linearität erhält man also:

<span>
     
        $$Ax=A(c_1e_1&#43;\cdots &#43;c_ne_n)=c_1Ae_1&#43;\cdots &#43; c_nAe_n$$
    
</span>
</p>
<p>Aus der Interpretation von Matrizen als lineare Transformationen ergibt sich folgendes Bild:</p>
<ul>
<li>
<p>Die Elementarmatrizen entsprechen Basistransformationen (Rotation, Verschieben, Stauchen oder Strecken) und dass jede Matrix als Produkt von Elementarmatrizen darstellbar ist, bedeutet dass eine Transformation als Komposition von Basistransformationen beschrieben werden kann.</p>
</li>
<li>
<p>Eine Diagonalmatrix stellt einer Transformation dar, die die Basisvektoren nur skaliert.</p>
</li>
<li>
<p>Die <strong>Matrizenmultiplikation</strong> 
<span>
                  
        \(AB\)
    
</span>
 entspricht der Komposition der beiden Transformationen (
<span>
                  
        \(A\circ B\)
    
</span>
, d.h. erst wird 
<span>
                  
        \(B\)
    
</span>
 ausgeführt, dann 
<span>
                  
        \(A\)
    
</span>
).</p>
</li>
<li>
<p>Die <strong>Inverse</strong> einer Matrix entspricht der inversen Transformation:</p>
<ul>
<li>
<span>
                  
        \(Ax=v\)
    
</span>
 bedeutet, dass die Tranformation 
<span>
                  
        \(A\)
    
</span>
 den Vektor 
<span>
                  
        \(x\)
    
</span>
 auf den Vektor 
<span>
                  
        \(v\)
    
</span>
 abbildet.</li>
<li>Daraus folgt 
<span>
                  
        \(x=A^{-1}v\)
    
</span>
, d.h. man findet 
<span>
                  
        \(x\)
    
</span>
, indem man die inverse Transformation 
<span>
                  
        \(A^{-1}\)
    
</span>
 auf 
<span>
                  
        \(v\)
    
</span>
 anwendet.</li>
</ul>
</li>
<li>
<p>Der <strong>Rang</strong> einer Matrix entspricht der Anzahl des Dimensionen des Outputs der Transformation.</p>
</li>
<li>
<p>Die <strong>Determinante</strong> einer Matrix ist der Faktor, um den ein Teil des Raumes durch die Transformation gestaucht oder gestreckt wird (z.B. der Inhalt einer Fläche im zweidimensionalen Raum oder das Volumen im dreidimensionalen Raum).</p>
<ul>
<li>Ist die Determinante negativ, entspricht das einer Umkehrung der Orientierung des Raumes.</li>
<li>Ist die Determinante 0, heißt das, die Transformation bildet auf eine niedrigere Dimension ab. Man verliert also Informationen und kann die Transformation deswegen nicht rückgängig machen, d.h. die Matrix ist nicht invertierbar.</li>
</ul>
</li>
</ul>
<p>Jede Matrix ist die Darstellung eines Endomorphismus des Vektorraums (mit Ähnlichkeit als Äquivalenzrelation und den entsprechenden Normalformen) und einer Bilinearform des Vektorraums (mit Kongruenz als Äquivalenzrelation und den entsprechenden Normalformen).</p>
<h1 id="matrizen-als-gleichungssysteme">Matrizen als Gleichungssysteme</h1>
<h1 id="literatur">Literatur</h1>
<ul>
<li>Luise Unger: Skript Lineare Algebra (1143), FernUniversität Hagen.</li>
<li>Klaus Jänich: Lineare Algebra.</li>
</ul>


    
    </div>
  </div>
</main>
  <footer> <footer id="footer">
  <nav>
    <ul>
      <li><span>2022 (c) Christina Unger</span></li>
      <li>
        Rendered with <a href="https://gohugo.io/">Hugo</a>. Theme based on <a href="https://kube.elemnts.net/">Kube</a>.
      </li>
    </ul>
  </nav>
</footer>
 </footer>

  <script src="/js/kube.js" type="text/javascript">
  </script>
  <script src="/js/kube.legenda.js" type="text/javascript">
  </script>
  <script src="/js/master.js" type="text/javascript">
  </script>
</body>

</html>
